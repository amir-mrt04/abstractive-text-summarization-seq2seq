{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCJAa3_TDRjl",
    "outputId": "d60d77c9-e46e-4d9c-ebfb-41359ab3d02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_sTAJJCDJp_"
   },
   "outputs": [],
   "source": [
    "rm -rf ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653,
     "referenced_widgets": [
      "01d6796aba334bb8a0a35d3fc8aff79b",
      "dbc3ba80ae6f469f87de541cdbf9f60c",
      "e59564154a704b1c92e96a8cab1a5b2a",
      "78930c80b4614ab7bf745b2a7ef68389",
      "f3c7689ea15d42b8b92abe1fd45c6504",
      "65df6a735dd94fbd860f5b90f3b90e4a",
      "56276edfd9fe4ae28f203211390f52da",
      "b44f3a6f4d144833a9c9f8fe56da662d",
      "3ce3179a946b4f9896f77ce1e7a9f0e0",
      "af31206e2ab64ba5a1d16460db257426",
      "9bc83f122dfb4c549d422c678ec7674d",
      "4315864f28b64069ad8b00dc86effbd5",
      "e423d3119175450eb8a02d8e10d2188e",
      "2eb86694a90e4b52a49ad97078fdcf51",
      "6858190c4b3945f0b46c908692b75dd0",
      "6a9ab67c457b4f77abfcdde9b6eea1af",
      "d51030d1d70a46489e079350609d7f04",
      "db297830eedf41d7ac60b852c8c9e2b6",
      "4777da5d01124508a43ca91db2bdf4fd",
      "8a5a946c286441bdbc87895eba6a30e4",
      "a355d176bf394043a46bbbfecec45d68",
      "b1c5370b4d874d26a8c97e6d46c789a4",
      "57a7d7308d85405ba178a05657aee3c5",
      "cb84aa8d5735441ebf984a14293cd011",
      "64628dd2cad44629b3cb4629c7398df0",
      "c5634e4be42841ada4a906d59cba2e2b",
      "0d68678086984dcea6f3122082529dec",
      "708586a2709046349857e33b444010b0",
      "d8b91fbb35244ab2ab58db49efb6365f",
      "94b66ea616a24d25a6680d7a3a1e2e4d",
      "ef4d2d228ca64440b391ade4a70466ae",
      "a5135961372b4a199da2c691eb423fde",
      "99b58291187643ff953ad2662a154ff8",
      "0a57e28b1c944f039db6faf0d2050934",
      "695a1f7a3e224ba0966676ce08b63ac9",
      "66c901a228d847f5898832eb8627d840",
      "16d6fe62634d4495b23897d1292464aa",
      "8afa93c010dd497faafcdf2ae4e85489",
      "53f5ff11e9464321878f7c607c1c12d3",
      "0bcdfc356cec4b488bd690a8b5055ad3",
      "95eb6bea60e04596aa218355a895cc6e",
      "101f97c3a6d9402bb5d30bfaac25ac7f",
      "dd369c35042a4ecc9f9a9fd763c52cbf",
      "6f8a7cbe7fb845828bbaf87ac9669c52",
      "30d38fbc0488415ba527592eac0bc9c8",
      "16ea488acf2648e988aefb2b29b02c42",
      "c4fa0993d3ec4f9d8d2281f4ff122653",
      "167ea80e69e84b3cbd6b9b0c1a5faf4f",
      "5dddc1cb04e64fe8bd157e239a044838",
      "9c65145ef59e4571a318a3229d77f149",
      "eabf479dbd11465c8c6e30393cf69f53",
      "1dae9e6056054d08a6568fc3c79dd59a",
      "5dc02ea87a4a41319982fe55769eeef5",
      "7f0b0a31205e43a98323031230494818",
      "af33d8c012bc4e30814fcd1fa1715783",
      "22ba5dad26de4aa6a9bba5cc5f0d108c",
      "dcb55a3f18bc432dab2a07ca1566605f",
      "42bb8e85902840118b36dda2c9034e56",
      "3f12b37741ce47478ebb6704d518d14c",
      "d49fc5511b8b4920bfc0ef49d847158d",
      "68dc5300a2354d5999527dceb4028f5a",
      "bca66b45402742f3ade26d2f0d683c7b",
      "8aab85885bba42af8c4dc0cfef51cbd4",
      "236fdd67caf24108b01deb280791fea6",
      "8dcc9c816d1c4516a8d12b9fcf73215e",
      "0128ce38a48f4920a7b685e26af92bee",
      "10d13addde6b453f8c79bfb653f971bc",
      "655987bec72142dc8aa6207f4b622e07",
      "a9eae9824a0341e4bcee103884a89e0b",
      "0c078eb04a1644b0b1d80180746a42b4",
      "62a1442c381a440fbf6dc72068818533",
      "c6ea3507d8cb4261ac977bbc43192d25",
      "bce561c134984beaa2b144fd4b705e25",
      "648591c23eeb40c0b87ea47df3cf024f",
      "bdcacd7cf97348ab9db91df7bce52c0c",
      "8ed6de779b3b4adca5164e1394f1e7b2",
      "2e4a5780ef7447089a50d4f77d407f5d",
      "128591e087c6446d80234716f50e9ec7",
      "7e60676a7ecf4b4e87dc93959ea9c5c2",
      "52a8056c06634226ab09d0a181424848",
      "abdf1c1f6b4a4855b3a0e993c4d1580b",
      "5f0188656ba04e86b38d3bd01113aeb9",
      "80bf3a52d194406886f132a3fea5a3c8",
      "14ebf82c8e584617b97dc96882f735e9",
      "53a0565e588a45cabfa895166508c6a9",
      "ad688e199052407b852bb5c123798d21",
      "7e31bfe218724f83bb1dd8379042caaf",
      "c402325c1091433cbf9b171325b13d03",
      "fbb4bb78bb444369b7e58a906299d6f8",
      "162e66df900c4fc3b2f2eb0f44e46d06",
      "93aed55b3b39466fb2fc64fe296269a5",
      "295991a2d27649e29f60b75e11cf4f33",
      "ac6a369f7620434cadc3605896261576",
      "abd9f9f190214c1b98257deae8cdfaa3",
      "1b67c62eccac4f45bc0d9976a1d3c25d",
      "9f035acfcb9548cc9985cb2ad0e90c6a",
      "5f546d1bc31f472f8762b2b352dcfecd",
      "c4c7b2ae093e445a9b8c462af2e0bcb6",
      "eb6d7d69be6e4ef0b483c571389896a1",
      "364be245dc9649f6a38333fba407ae88",
      "f659b011fa8a42f78aa784a1c5236849",
      "5695e1dc5501423d8addfe6e2365a4fb",
      "eda5edf9451245ae92da0a33b3573808",
      "f58a9663353c439593601dbbbec403fd",
      "b4322762fbbf4102a12f06573318490b",
      "c54387c7937448f8a5a4fe4ff825d382",
      "8abe9daaa1d3428c9ed4d42623da25f6",
      "6d8eafbbb00d4918b5e1fe5dd8f23e30",
      "94053e121cd84013a564b52011416818",
      "8bf2f959563d4d5b9ac5cca7f27e03de",
      "4f9b7edc1a624cb68ab2c201f5a33b4d",
      "9e0f62381096403ba9e1edd091b779d3",
      "cb5030aa524b48d99fc31cb284a96a0b",
      "0fc2f3ead30143dc90975e7eb4c13f02",
      "5354dd64d3aa446097258eaf36d168eb",
      "c8d1214908704f49a8fdeb84ec3c62b1",
      "04a188f7d1824fb682c0a135cf2fc9ce",
      "21b9e7485f6d4ad98b5f62dcbc705dc0",
      "a0a8d715099a4b36a24a17f98a9458e9",
      "f04dba90d3644ee8b7fc94da09ccdb18",
      "13f37912b0af4a96a2edce40684bf293",
      "7001ff96f34940c8b216444f0158f4d9",
      "011233b57efe4e7d966fc3249b2b7fc2",
      "6af49ce4f2ff43748ed7d54e57de99e2",
      "016203a2154a42d3805c2617412d24e3",
      "920522cd72804a34b624e1c07f2b90e2",
      "f325d4a9fa8f4ca0b04ea69d42cb3be1",
      "1d247e8160dd4a0aa50f705fcb53c428",
      "89b007f8bdf140558f9d8da56b8dc855",
      "378988b53a3043be8a58859d531d8917",
      "a20185f883994e12808587dc44ced8ed",
      "2845a71e9adc41cdaf32c47db565ffaa",
      "2cb5cc7f7d884bcb8167434e9328c103",
      "c287a34b931c4e9b9591109fe20752e5",
      "b28ee1e0788343cfa7df22fe4da6d80b",
      "2ce1d9eac04249fb98df7cff44950d45",
      "dfc146910bc44a73bbfc22434f0e7501",
      "655921722b6c4644b40f69539291d6bf",
      "9abb1fbecc60482791fd55773a769a66",
      "bee1b667f97c4a55a77d39f0981068de",
      "76e4640ef35b4c139b99977a0485a19a",
      "e3adc2f268014a58aa15c5ec489a2e0f",
      "4a7db42303484b6a8cf6b6332bdd109f",
      "df3d4a1dfe2e46ac8c943318b12f6d77",
      "e24445535c2942eb89410f0c2709e2e1",
      "a939d0ad88284c4da5ba2b92bd470a8c",
      "23f96a5c6b074f3394e1097afd01fb05",
      "688e8844f5324e9ea6b0c547f79ad639",
      "e1a8c880769b4753a599ff2c59b2a660",
      "4760348614ba4237aa323a41388236bd",
      "a0076e53b1194856991558ee83e104f4",
      "6a2b5e2e1ca84087a8a96b2a4f98cf96",
      "cd0e7ae0c209483bae755c0054d0dc6a",
      "4811ad5fcc544dcda73249204aa2106f",
      "489249ae1030469f8c890e23c980d743",
      "d7f6c4e0bd4c49c88ecb96e1ce8137dd",
      "0efa67b6d838465abf03993cd1d7d8c8",
      "0bbe7f4c54f14d639d1dbcce14aca468",
      "6a517bbe628f4712aad2045645e083aa",
      "fe213b07771644db93f287f4118e36ce",
      "d3c506f067454bc1a445e7e8e6469aa1",
      "72a4a560015f4cd1b9be59f74c8686c8",
      "0d1f0c5ba89047128c34d021a2bc961f",
      "a916e521361d482087603b3c4ff2a05b",
      "728de672fc944ca7a034bdb0f579373d",
      "83d4814f05e8485b9bb082833f0743e8",
      "0a93fa10d32045e684c70643cb5d6a86",
      "8e3e530b65244e16b43bbf1ab984fa90",
      "3e98ad7d2b994e539cf2bfc53b8ed712",
      "9abcdbdf3c1e4f40b154b62b274d48e1",
      "ea17a58768894b17bf512f35a896cebf",
      "87ad4201cf8b47fa875ed2cc9edfc5f6",
      "247b953333bb40edbb5ac12fe13bfa5f",
      "461b19c1872f432497053005032e6864",
      "af7d4d89ee7f4dc899f6daf932ccb03e",
      "d7ee447159304a2393777286a6031250"
     ]
    },
    "id": "Dan4wBy_BrJ6",
    "outputId": "daaf6374-573b-4ef5-f205-151a335d9869"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "train_dataset = dataset['train'].select(range(1000))  # 3000 نمونه\n",
    "val_dataset = dataset['validation'].select(range(200))  # 500 نمونه\n",
    "test_dataset = dataset['test'].select(range(100))\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess_data(example):\n",
    "    article = example['article']\n",
    "    summary = example['highlights']\n",
    "    inputs = tokenizer(article, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    outputs = tokenizer(summary, max_length=150, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": outputs[\"input_ids\"].squeeze()\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dm9Q2KyB46G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, attention_mask):\n",
    "        # hidden: [batch_size, hidden_size]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        batch_size, seq_len, _ = encoder_outputs.size()\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_size]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, seq_len, hidden_size]\n",
    "        energy = energy.matmul(self.v)  # [batch_size, seq_len]\n",
    "        energy = energy.masked_fill(attention_mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(energy, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        context = attn_weights * encoder_outputs  # [batch_size, seq_len, hidden_size]\n",
    "        context = context.sum(dim=1)  # [batch_size, hidden_size]\n",
    "        return context, attn_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden, cell, encoder_outputs, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids.unsqueeze(1)))  # [batch_size, 1, embed_size]\n",
    "        context, attn_weights = self.attention(hidden[-1], encoder_outputs, attention_mask)\n",
    "        lstm_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # [batch_size, 1, embed_size + hidden_size]\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        output = self.fc(output.squeeze(1))  # [batch_size, vocab_size]\n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_ids, input_mask, target_ids):\n",
    "        encoder_outputs, hidden, cell = self.encoder(input_ids, input_mask)\n",
    "        outputs = []\n",
    "        input_token = target_ids[:, 0]\n",
    "\n",
    "        for t in range(1, target_ids.size(1)):\n",
    "            output, hidden, cell, _ = self.decoder(input_token, hidden, cell, encoder_outputs, input_mask)\n",
    "            outputs.append(output)\n",
    "            input_token = target_ids[:, t]\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soZGYJzCDmkd",
    "outputId": "78de49e5-3a11-40b3-8e41-539ce60d7b16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  17%|█▋        | 131/750 [02:27<11:24,  1.11s/batch, batch_loss=7.73, avg_loss=8.14]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, labels)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        labels = labels[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item(), \"avg_loss\": epoch_loss / (progress_bar.n + 1)})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed, Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkIfC1GAD73l"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def generate_summary(model, tokenizer, input_ids, attention_mask, max_length=150):\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    encoder_outputs, hidden, cell = model.encoder(input_ids, attention_mask)\n",
    "    generated = [tokenizer.bos_token_id]\n",
    "    for _ in range(max_length):\n",
    "        input_token = torch.tensor([generated[-1]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs, attention_mask)\n",
    "        next_token = output.argmax(dim=-1).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for example in dataset:\n",
    "        input_ids = example[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = example[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        pred = generate_summary(model, tokenizer, input_ids, attention_mask)\n",
    "        ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "\n",
    "\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    return {\n",
    "        'rouge1': np.mean(rouge1_scores),\n",
    "        'rouge2': np.mean(rouge2_scores),\n",
    "        'rougeL': np.mean(rougeL_scores)\n",
    "    }\n",
    "\n",
    "# ارزیابی\n",
    "rouge_scores = evaluate_model(model, tokenizer, test_dataset)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHsgDLN8YapS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# ایجاد پوشه برای ذخیره مدل\n",
    "save_dir = \"./bart-summarizer-final\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ذخیره مدل و توکنایزر\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"model.pt\"))\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtWlhLwJeGFg"
   },
   "source": [
    "{'id': '0054d6d30dbcad772e20b22771153a2a9cbeaf62',\n",
    " 'article': '(CNN) -- An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour.'\n",
    " 'highlights': 'The elderly woman suffered from diabetes and hypertension, ship's doctors say .\\nPreviously, 86 passengers had fallen ill on the ship, Agencia Brasil says .'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeEFKRSVYhgD"
   },
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=150):\n",
    "\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "    summary = generate_summary(model, tokenizer, input_ids, attention_mask, max_length=max_length)\n",
    "    return summary\n",
    "\n",
    "article = \"(CNN) -- An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour.\"\n",
    "summary = summarize_text(article, model, tokenizer)\n",
    "print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
